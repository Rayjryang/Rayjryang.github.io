<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jinrui Yang</title>

  <meta name="author" content="Jinrui Yang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">

  <script type="text/javascript">

    function display(id) {
      var traget = document.getElementById(id);
      if (traget.style.display == "none") {
        traget.style.display = "";
      } else {
        traget.style.display = "none";
      }
    }  
  </script>
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Jinrui Yang<img style="max-height:30px;vertical-align: middle;"/></name>

                  </p>
                  <p>
                    I am a second-year (starting from 2023) Ph.D. student at <a href="https://engineering.ucsc.edu/departments/computer-science-and-engineering/">University of California, Santa Cruz</a>, supervised by Prof. <a
                    href="https://cihangxie.github.io/">Cihang Xie</a> and Prof. <a
                    href="https://yuyinzhou.github.io/">Yuyin Zhou</a>.

                    Before joining UCSC, I was a researcher at  
                    <a href="https://www.tencent.com/en-us/about.html">Tencent</a>.
          
                    Previously, I got the Master degree at 
                    <a href="https://www.sysu.edu.cn/sysuen/">SUN YAT-SEN UNIVERSITY</a> in 2021, supervised by Prof.  <a
                    href="https://www.isee-ai.cn/~zhwshi/">Wei-Shi Zheng</a>.
                    Before that, I received my B.Eng. from <a href="https://en.scu.edu.cn/">Sichuan University</a> in 2019. 
                  </p>


                  <p>  
                  My research interests lie in deep learning and computer vision. I have published several works on image and video perception as well as multimodal learning. Currently, I focus on generative AI and foundation model training.
                  </p>
                  
                  
                  <p>
                    Feel free to reach out to me at 
                    <a href="mailto:jinruiyang.ray@gmail.com" style="color:blue;">jinruiyang.ray@gmail.com</a> 
                    for discussions or opportunities. 
                    <span style="color:red;">I'm looking for research intern positions for Summer 2025.</span>
                  </p>
                
                  <p>
                  </p>
                  <p style="text-align:center">
                    <a href="data/cv.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=8iBtvCQAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/Rayjryang">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/photo-life.JPG"><img style="width:80%;max-width:80" alt="profile photo"
                      src="images/photo-life.JPG" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"><tbody>
          <tr>
            <td style="padding:1%;width:100%;vertical-align:middle">
              <heading>Work Experiences</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="border-spacing:10px 0px;">
          <tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/adobe.jpg", width="80">
              </td>
              <td width="75%" valign="center">
                <papertitle>Adobe Research</papertitle> <br>
                Research Intern, 2024.6 - 2025.5 (Expected) <br>
                Advisor: <a href="https://qliu24.github.io/" target="blank_">Qing Liu</a> and <a href="https://research.adobe.com/person/zhe-lin/" target="blank_">Zhe Lin</a> <br>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/tencent.jpg", width="100">
              </td>
              <td width="75%" valign="center">
                <papertitle>Tencent YouTu Lab</papertitle> <br>
                Researcher, 2021.7 - 2023.8 <br>
              </td>
            </tr>
          </tbody>
        </table>
          


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <ul>
                    <li> <b>[Feb. 2025]</b> One paper accepted by CVPR2025!</li>
                    <li> <b>[Oct. 2024]</b> One paper accepted by NeurIPS2024!</li>
                    <li> <b>[Jun. 2024]</b> I join Adobe Research as a research intern.</li>
                    <li> <b>[Sept. 2023]</b> I join UCSC CSE as a PhD student. </li>
                    </div>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Selected Research
                  <p>
                    <a href="https://scholar.google.com/citations?user=8iBtvCQAAAAJ&hl=en">Google Scholar</a> 
                  </p>

                  <p>
                    (*: Equal contribution)
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>


              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/LayerDecomp_teaser.png' width="190">
                    </div>
                    <img src='images/LayerDecomp_teaser.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }
                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://rayjryang.github.io/LayerDecomp/">
                    <papertitle>Generative Image Layer Decomposition with Visual Effects</papertitle>
                  </a>
                  <br>
                  <strong>Jinrui Yang</strong>, Qing Liu, Yijun Li, Soo Ye Kim, Shilong Zhang, Daniil Pakhomov, Mengwei Ren, Jianming Zhang, Zhe Lin, Cihang Xie, Yuyin Zhou
                  <br>
                  <em>CVPR</em>, 2025
                  <br>
                  <a href="https://arxiv.org/abs/2411.17864">paper</a>/
                  <a href="https://rayjryang.github.io/LayerDecomp/">page</a>
                  <p></p>
                  <p>
                    LayerDecomp outputs photorealistic clean backgrounds and high-quality transparent foregrounds with faithfully preserved visual effects. 
                  </p>
                </td>
              </tr>



  

              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/crate_alpha_teaser.png' width="190">
                    </div>
                    <img src='images/crate_alpha_teaser.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }
                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2405.20299">
                    <papertitle>Scaling White-Box Transformers for Vision</papertitle>
                  </a>
                  <br>
                  <strong>Jinrui Yang</strong>, Xianhang Li, Druv Pai, Yuyin Zhou, Yi Ma, Yaodong Yu, Cihang Xie
                  <br>
                  <em>NeurIPS</em>, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2405.20299">paper</a>/
                  <a href="https://rayjryang.github.io/CRATE-alpha/">page</a>/
                  <a href="https://github.com/UCSC-VLAA/CRATE-alpha">code</a>/
                  <a href="https://mp.weixin.qq.com/s/0Ps_9BVHDulpEprlb-3sgg">新智元</a>
                  <p></p>
                  <p>
                    We scaled the white-box transformer architecture, resulting in the CREATE-α model, which significantly improves upon the vanilla white-box transformer while preserving interpretability. This advancement nearly closes the gap between white-box transformers and ViTs.
                  </p>
                </td>
              </tr>


              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/MME_teaser.png' width="190">
                    </div>
                    <img src='images/MME_teaser.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }
                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2306.13394">
                    <papertitle>MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models</papertitle>
                  </a>
                  <br>
                  Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, <strong>Jinrui Yang</strong>, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, Rongrong Ji
                  <br>
                  <em>Preprint</em>, 2023
                  <br>
                  <a href="https://arxiv.org/abs/2306.13394">paper</a>/
                  <a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">page</a>/
                  <a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">code</a>/
                  <a href="https://mp.weixin.qq.com/s/kQq0_sg3PQVVzzuud5Ri9Q">新智元</a>
                  <p>
                  The paper introduces the first comprehensive Multimodal Large Language Model (MLLM) evaluation benchmark, MME. It measures both perception and cognition abilities across 14 subtasks, with 30 advanced MLLMs evaluated comprehensively on MME.
                  </p>
                </td>
              </tr>


            
              
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/iccv21_teaser.png' width="190">
                    </div>
                    <img src='images/iccv21_teaser.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }
                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Learning_To_Know_Where_To_See_A_Visibility-Aware_Approach_for_ICCV_2021_paper.pdf">
                    <papertitle>Learning To Know Where To See: A Visibility-Aware Approach for Occluded Person Re-Identification</papertitle>
                  </a>
                  <br>
                  <strong>Jinrui Yang</strong>, Jiawei Zhang, Fufu Yu, Xinyang Jiang, Mengdan Zhang, Xing Sun, Ying-Cong Chen, Wei-Shi Zheng
                  <br>
                  <em>ICCV</em>, 2021
                  <br>
                  <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Learning_To_Know_Where_To_See_A_Visibility-Aware_Approach_for_ICCV_2021_paper.pdf">paper</a>
                  <p></p>
                  <p>
                  We propose a novel method to discretize pose information into visibility labels for body parts, ensuring robustness against sparse and noisy pose data in Occluded Person Re-ID.
                  </p>
                </td>
              </tr>      


              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/STGCN_teaser.png' width="190">
                    </div>
                    <img src='images/STGCN_teaser.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Spatial-Temporal_Graph_Convolutional_Network_for_Video-Based_Person_Re-Identification_CVPR_2020_paper.pdf">
                    <papertitle>Spatial-temporal graph convolutional network for video-based person re-identification</papertitle>
                  </a>
                  <br>
                  <strong>Jinrui Yang</strong>, Wei-Shi Zheng, Qize Yang, Ying-Cong Chen, Qi Tian
                  <br>
                  <em>CVPR</em>, 2020

                  <br>
                  <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Spatial-Temporal_Graph_Convolutional_Network_for_Video-Based_Person_Re-Identification_CVPR_2020_paper.pdf">paper</a>
                  <p>
                   We propose a novel Spatial-Temporal Graph Convolutional Network (STGCN) to address the occlusion problem and the visual ambiguity issue caused by visually similar negative samples in video-based person Re-ID.
                  </p>
                </td>
              </tr>
              

            </tbody>
          </table>



          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Academic Service</heading>
                  <p>
                    <b>Reviewer</b>
                  </p>
                  <ul>
                    <li> Conference: ICML 2024, CVPR 2024, WACV 2023</li>
                    <li> Journals: TIP, TCSVT, TMM, ect</li>
                </td>
              </tr>
            </tbody>
          </table>

           
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Teaching </heading>
                  <ul>
                    <li> CSE 144 - Applied Machine Learning: Deep Learning, Winter 2024.</li>
                    <li> CSE 290D - Neural Computation, Fall 2025.</li>
                </td>
              </tr>
            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:10%;vertical-align:middle">
                  <script type='text/javascript' id='clustrmaps' src='//clustrmaps.com/map_v2.js?d=YbOnbv6wDgVcDB9Jg0khAwMHT-EtVsHnUrcq6JJQ0R8&cl=ffffff'></script>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's
                    website</a>
                </p>
              </td>
            </tr>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>